{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from nets.NetOneLayer import NetOneLayer\n",
    "from nets.NetOneLayerLowRank import NetOneLayerLowRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "batch_size_test = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.406208\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 2.333980\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 2.300105\n",
      "\n",
      "Test set: Average loss: -0.1719, Accuracy: 1751/10000 (18%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.223307\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.199644\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.206126\n",
      "\n",
      "Test set: Average loss: -0.2552, Accuracy: 2604/10000 (26%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.185379\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.140334\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.169295\n",
      "\n",
      "Test set: Average loss: -0.3296, Accuracy: 3418/10000 (34%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.133851\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.103420\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.079953\n",
      "\n",
      "Test set: Average loss: -0.3608, Accuracy: 3639/10000 (36%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.107602\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.004951\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.058230\n",
      "\n",
      "Test set: Average loss: -0.3744, Accuracy: 3763/10000 (38%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.072062\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.079653\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.091360\n",
      "\n",
      "Test set: Average loss: -0.3864, Accuracy: 3900/10000 (39%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.999546\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 2.077190\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 2.063553\n",
      "\n",
      "Test set: Average loss: -0.3943, Accuracy: 3956/10000 (40%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.036794\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 2.027719\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 2.066898\n",
      "\n",
      "Test set: Average loss: -0.4246, Accuracy: 4287/10000 (43%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.972914\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.998944\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 2.060379\n",
      "\n",
      "Test set: Average loss: -0.4522, Accuracy: 4565/10000 (46%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.046840\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 2.108898\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.993987\n",
      "\n",
      "Test set: Average loss: -0.4626, Accuracy: 4669/10000 (47%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.968445\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 2.048425\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 1.919159\n",
      "\n",
      "Test set: Average loss: -0.4724, Accuracy: 4764/10000 (48%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 1.981577\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 2.016019\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 1.955661\n",
      "\n",
      "Test set: Average loss: -0.4810, Accuracy: 4842/10000 (48%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 1.926030\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 1.934478\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 2.000781\n",
      "\n",
      "Test set: Average loss: -0.4845, Accuracy: 4890/10000 (49%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 1.982365\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 1.985215\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 2.057292\n",
      "\n",
      "Test set: Average loss: -0.4890, Accuracy: 4920/10000 (49%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 1.906085\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 1.942768\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 1.913983\n",
      "\n",
      "Test set: Average loss: -0.4929, Accuracy: 4964/10000 (50%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 2.021034\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 1.992924\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 1.990603\n",
      "\n",
      "Test set: Average loss: -0.4944, Accuracy: 4983/10000 (50%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 1.969316\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 1.959268\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 1.998496\n",
      "\n",
      "Test set: Average loss: -0.4998, Accuracy: 5016/10000 (50%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 1.988235\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 2.010019\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 2.032568\n",
      "\n",
      "Test set: Average loss: -0.5029, Accuracy: 5058/10000 (51%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 1.931926\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 1.920751\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 1.928523\n",
      "\n",
      "Test set: Average loss: -0.5040, Accuracy: 5058/10000 (51%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 1.983374\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 1.933453\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 2.048582\n",
      "\n",
      "Test set: Average loss: -0.5030, Accuracy: 5051/10000 (51%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 1.904322\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 1.877352\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 1.967324\n",
      "\n",
      "Test set: Average loss: -0.5046, Accuracy: 5087/10000 (51%)\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 1.908999\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 1.902710\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 1.961313\n",
      "\n",
      "Test set: Average loss: -0.5045, Accuracy: 5083/10000 (51%)\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 1.923593\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 1.959358\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 1.941014\n",
      "\n",
      "Test set: Average loss: -0.5069, Accuracy: 5091/10000 (51%)\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 1.912803\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 1.882223\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 1.954436\n",
      "\n",
      "Test set: Average loss: -0.5093, Accuracy: 5134/10000 (51%)\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 1.957276\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 1.919260\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 1.949041\n",
      "\n",
      "Test set: Average loss: -0.5074, Accuracy: 5109/10000 (51%)\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 1.921463\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 1.971375\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 1.915862\n",
      "\n",
      "Test set: Average loss: -0.5112, Accuracy: 5137/10000 (51%)\n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 2.025135\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 1.920524\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 1.993363\n",
      "\n",
      "Test set: Average loss: -0.5111, Accuracy: 5159/10000 (52%)\n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 1.917298\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 1.951174\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 1.962984\n",
      "\n",
      "Test set: Average loss: -0.5129, Accuracy: 5149/10000 (51%)\n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 2.029766\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 1.948402\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 1.977802\n",
      "\n",
      "Test set: Average loss: -0.5140, Accuracy: 5183/10000 (52%)\n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 1.963862\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 1.898440\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 1.923999\n",
      "\n",
      "Test set: Average loss: -0.5097, Accuracy: 5132/10000 (51%)\n",
      "\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 1.893429\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 2.021485\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 1.880234\n",
      "\n",
      "Test set: Average loss: -0.5135, Accuracy: 5184/10000 (52%)\n",
      "\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 1.961091\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 1.901108\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 2.013444\n",
      "\n",
      "Test set: Average loss: -0.5125, Accuracy: 5166/10000 (52%)\n",
      "\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 1.791877\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 1.976090\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 1.933115\n",
      "\n",
      "Test set: Average loss: -0.5149, Accuracy: 5179/10000 (52%)\n",
      "\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 1.941243\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 1.880182\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 1.929795\n",
      "\n",
      "Test set: Average loss: -0.5169, Accuracy: 5203/10000 (52%)\n",
      "\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 1.961117\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 1.893106\n"
     ]
    }
   ],
   "source": [
    "lr = 0.02\n",
    "momentum = 0.9\n",
    "n_epochs = 50\n",
    "\n",
    "model = NetOneLayerLowRank(n_hidden=2**8, d=2, K=2)\n",
    "# model = NetOneLayer(n_hidden=2**8)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8301, 0.2546, 0.5547, 0.2650]],\n",
       "\n",
       "        [[0.4740, 0.0082, 0.6353, 0.9605]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2,1, 4)\n",
    "a\n",
    "F.max_pool1d(a, 4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Applies a 2D max pooling over an input signal composed of several input\n",
       "planes.\n",
       "\n",
       "In the simplest case, the output value of the layer with input size :math:`(N, C, H, W)`,\n",
       "output :math:`(N, C, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kH, kW)`\n",
       "can be precisely described as:\n",
       "\n",
       ".. math::\n",
       "\n",
       "    \\begin{equation*}\n",
       "    \\text{out}(N_i, C_j, h, w)  = \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1}\n",
       "                           \\text{input}(N_i, C_j, \\text{stride}[0] * h + m, \\text{stride}[1] * w + n)\n",
       "    \\end{equation*}\n",
       "\n",
       "If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides\n",
       "for :attr:`padding` number of points. :attr:`dilation` controls the spacing between the kernel points.\n",
       "It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.\n",
       "\n",
       "The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n",
       "\n",
       "    - a single ``int`` -- in which case the same value is used for the height and width dimension\n",
       "    - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n",
       "      and the second `int` for the width dimension\n",
       "\n",
       "Args:\n",
       "    kernel_size: the size of the window to take a max over\n",
       "    stride: the stride of the window. Default value is :attr:`kernel_size`\n",
       "    padding: implicit zero padding to be added on both sides\n",
       "    dilation: a parameter that controls the stride of elements in the window\n",
       "    return_indices: if ``True``, will return the max indices along with the outputs.\n",
       "                    Useful when Unpooling later\n",
       "    ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(N, C, H_{in}, W_{in})`\n",
       "    - Output: :math:`(N, C, H_{out}, W_{out})` where\n",
       "\n",
       "      .. math::\n",
       "          H_{out} = \\left\\lfloor\\frac{H_{in} + 2 * \\text{padding}[0] - \\text{dilation}[0]\n",
       "                * (\\text{kernel_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n",
       "\n",
       "          W_{out} = \\left\\lfloor\\frac{W_{in} + 2 * \\text{padding}[1] - \\text{dilation}[1]\n",
       "                * (\\text{kernel_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> # pool of square window of size=3, stride=2\n",
       "    >>> m = nn.MaxPool2d(3, stride=2)\n",
       "    >>> # pool of non-square window\n",
       "    >>> m = nn.MaxPool2d((3, 2), stride=(2, 1))\n",
       "    >>> input = torch.randn(20, 16, 50, 32)\n",
       "    >>> output = m(input)\n",
       "\n",
       ".. _link:\n",
       "    https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\anaconda3\\envs\\main-env\\lib\\site-packages\\torch\\nn\\modules\\pooling.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?nn.MaxPool2d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main-env)",
   "language": "python",
   "name": "main-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
