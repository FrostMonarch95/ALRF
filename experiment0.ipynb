{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import utils\n",
    "from nets.NetOneLayer import NetOneLayer\n",
    "from nets.NetOneLayerLowRank import NetOneLayerLowRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "batch_size_test = 1000\n",
    "\n",
    "train_loader, test_loader = utils.load_mnist(batch_size, batch_size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.322811\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 2.222454\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 2.206626\n",
      "\n",
      "Test set: Average loss: -0.2225, Accuracy: 2244/10000 (22%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.248930\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.197760\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.251904\n",
      "\n",
      "Test set: Average loss: -0.3067, Accuracy: 3098/10000 (31%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.162878\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.140502\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.054999\n",
      "\n",
      "Test set: Average loss: -0.3801, Accuracy: 3821/10000 (38%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.142188\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.107588\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.926021\n",
      "\n",
      "Test set: Average loss: -0.3997, Accuracy: 4015/10000 (40%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.025115\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.069120\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.057216\n",
      "\n",
      "Test set: Average loss: -0.4136, Accuracy: 4155/10000 (42%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.062499\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.043024\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.022852\n",
      "\n",
      "Test set: Average loss: -0.4201, Accuracy: 4208/10000 (42%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.012036\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 1.977426\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 2.014513\n",
      "\n",
      "Test set: Average loss: -0.4275, Accuracy: 4287/10000 (43%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.060404\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 2.073262\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 2.004825\n",
      "\n",
      "Test set: Average loss: -0.4337, Accuracy: 4354/10000 (44%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.044182\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.972723\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 2.187994\n",
      "\n",
      "Test set: Average loss: -0.4371, Accuracy: 4391/10000 (44%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.978620\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.992837\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.910218\n",
      "\n",
      "Test set: Average loss: -0.4408, Accuracy: 4419/10000 (44%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 2.012911\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 2.099400\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 1.998971\n",
      "\n",
      "Test set: Average loss: -0.4435, Accuracy: 4450/10000 (44%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 2.015110\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 2.037216\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 2.005268\n",
      "\n",
      "Test set: Average loss: -0.4459, Accuracy: 4470/10000 (45%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 2.135767\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 2.015935\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 1.952716\n",
      "\n",
      "Test set: Average loss: -0.4496, Accuracy: 4502/10000 (45%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 2.082492\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 2.064328\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 2.008923\n",
      "\n",
      "Test set: Average loss: -0.4516, Accuracy: 4531/10000 (45%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 2.049428\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 1.952119\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 2.039026\n",
      "\n",
      "Test set: Average loss: -0.4527, Accuracy: 4536/10000 (45%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 1.956765\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 2.023370\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 2.016078\n",
      "\n",
      "Test set: Average loss: -0.4543, Accuracy: 4555/10000 (46%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 2.015710\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 1.965539\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 1.960950\n",
      "\n",
      "Test set: Average loss: -0.4571, Accuracy: 4585/10000 (46%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 2.022197\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 2.047734\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 2.008692\n",
      "\n",
      "Test set: Average loss: -0.4575, Accuracy: 4590/10000 (46%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 2.021383\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 1.993948\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 1.959820\n",
      "\n",
      "Test set: Average loss: -0.4584, Accuracy: 4592/10000 (46%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 2.055030\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 1.931220\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 1.971647\n",
      "\n",
      "Test set: Average loss: -0.4597, Accuracy: 4611/10000 (46%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 1.962153\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 2.045105\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 2.031616\n",
      "\n",
      "Test set: Average loss: -0.4602, Accuracy: 4614/10000 (46%)\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 2.022373\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 2.079202\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 1.996622\n",
      "\n",
      "Test set: Average loss: -0.4612, Accuracy: 4630/10000 (46%)\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 2.005957\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 2.049601\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 2.002950\n",
      "\n",
      "Test set: Average loss: -0.4622, Accuracy: 4634/10000 (46%)\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 1.901770\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 1.985634\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 1.990946\n",
      "\n",
      "Test set: Average loss: -0.4627, Accuracy: 4641/10000 (46%)\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 2.010700\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 2.099104\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 2.001315\n",
      "\n",
      "Test set: Average loss: -0.4624, Accuracy: 4639/10000 (46%)\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 1.978946\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 2.003371\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 2.074842\n",
      "\n",
      "Test set: Average loss: -0.4635, Accuracy: 4648/10000 (46%)\n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 1.937841\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 1.982324\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 1.974343\n",
      "\n",
      "Test set: Average loss: -0.4634, Accuracy: 4642/10000 (46%)\n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 1.964892\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 1.998442\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 1.985905\n",
      "\n",
      "Test set: Average loss: -0.4636, Accuracy: 4645/10000 (46%)\n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 1.967119\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 2.070708\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 2.055152\n",
      "\n",
      "Test set: Average loss: -0.4642, Accuracy: 4650/10000 (46%)\n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 1.961679\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 2.013751\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 2.061379\n",
      "\n",
      "Test set: Average loss: -0.4646, Accuracy: 4657/10000 (47%)\n",
      "\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 2.044975\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 2.036546\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 1.985455\n",
      "\n",
      "Test set: Average loss: -0.4647, Accuracy: 4662/10000 (47%)\n",
      "\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 1.989531\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 2.029438\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 1.999658\n",
      "\n",
      "Test set: Average loss: -0.4652, Accuracy: 4667/10000 (47%)\n",
      "\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 2.071898\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 1.968162\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 1.965483\n",
      "\n",
      "Test set: Average loss: -0.4654, Accuracy: 4666/10000 (47%)\n",
      "\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 1.979670\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 1.902612\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 1.991351\n",
      "\n",
      "Test set: Average loss: -0.4658, Accuracy: 4667/10000 (47%)\n",
      "\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 2.082547\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 2.029676\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 2.044651\n",
      "\n",
      "Test set: Average loss: -0.4662, Accuracy: 4671/10000 (47%)\n",
      "\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 1.952228\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 1.936726\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 1.986844\n",
      "\n",
      "Test set: Average loss: -0.4666, Accuracy: 4673/10000 (47%)\n",
      "\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 1.945922\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 2.028949\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 1.984597\n",
      "\n",
      "Test set: Average loss: -0.4663, Accuracy: 4673/10000 (47%)\n",
      "\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 1.996042\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 1.975617\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 2.042628\n",
      "\n",
      "Test set: Average loss: -0.4685, Accuracy: 4700/10000 (47%)\n",
      "\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 2.077644\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 2.014968\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 2.023555\n",
      "\n",
      "Test set: Average loss: -0.5168, Accuracy: 5174/10000 (52%)\n",
      "\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 1.860831\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 1.899966\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 1.872715\n",
      "\n",
      "Test set: Average loss: -0.5296, Accuracy: 5310/10000 (53%)\n",
      "\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 1.980156\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 1.947790\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 1.964221\n",
      "\n",
      "Test set: Average loss: -0.5351, Accuracy: 5366/10000 (54%)\n",
      "\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 1.899745\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 1.935789\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 1.930421\n",
      "\n",
      "Test set: Average loss: -0.5395, Accuracy: 5413/10000 (54%)\n",
      "\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 1.890793\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 1.918684\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 1.980954\n",
      "\n",
      "Test set: Average loss: -0.5419, Accuracy: 5439/10000 (54%)\n",
      "\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 1.935008\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 1.968991\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 1.851526\n",
      "\n",
      "Test set: Average loss: -0.5427, Accuracy: 5434/10000 (54%)\n",
      "\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 1.871188\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 1.927309\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 1.886914\n",
      "\n",
      "Test set: Average loss: -0.5453, Accuracy: 5470/10000 (55%)\n",
      "\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 1.932043\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 1.912539\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 1.918573\n",
      "\n",
      "Test set: Average loss: -0.5473, Accuracy: 5493/10000 (55%)\n",
      "\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 1.915499\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 1.890072\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 1.959890\n",
      "\n",
      "Test set: Average loss: -0.5489, Accuracy: 5512/10000 (55%)\n",
      "\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 1.880635\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 1.893493\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 1.967027\n",
      "\n",
      "Test set: Average loss: -0.5496, Accuracy: 5512/10000 (55%)\n",
      "\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 1.930232\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 1.951149\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 1.967964\n",
      "\n",
      "Test set: Average loss: -0.5498, Accuracy: 5506/10000 (55%)\n",
      "\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 1.976118\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 1.889015\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 1.858986\n",
      "\n",
      "Test set: Average loss: -0.5514, Accuracy: 5526/10000 (55%)\n",
      "\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 1.883084\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 1.937549\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 1.856135\n",
      "\n",
      "Test set: Average loss: -0.5520, Accuracy: 5535/10000 (55%)\n",
      "\n",
      "Train Epoch: 51 [0/60000 (0%)]\tLoss: 1.934580\n",
      "Train Epoch: 51 [25600/60000 (43%)]\tLoss: 1.845898\n",
      "Train Epoch: 51 [51200/60000 (85%)]\tLoss: 1.935485\n",
      "\n",
      "Test set: Average loss: -0.5521, Accuracy: 5528/10000 (55%)\n",
      "\n",
      "Train Epoch: 52 [0/60000 (0%)]\tLoss: 1.886349\n",
      "Train Epoch: 52 [25600/60000 (43%)]\tLoss: 1.914487\n",
      "Train Epoch: 52 [51200/60000 (85%)]\tLoss: 1.826518\n",
      "\n",
      "Test set: Average loss: -0.5531, Accuracy: 5545/10000 (55%)\n",
      "\n",
      "Train Epoch: 53 [0/60000 (0%)]\tLoss: 1.884958\n",
      "Train Epoch: 53 [25600/60000 (43%)]\tLoss: 1.908030\n",
      "Train Epoch: 53 [51200/60000 (85%)]\tLoss: 1.939722\n",
      "\n",
      "Test set: Average loss: -0.5532, Accuracy: 5544/10000 (55%)\n",
      "\n",
      "Train Epoch: 54 [0/60000 (0%)]\tLoss: 1.883340\n",
      "Train Epoch: 54 [25600/60000 (43%)]\tLoss: 1.818192\n",
      "Train Epoch: 54 [51200/60000 (85%)]\tLoss: 1.925038\n",
      "\n",
      "Test set: Average loss: -0.5540, Accuracy: 5559/10000 (56%)\n",
      "\n",
      "Train Epoch: 55 [0/60000 (0%)]\tLoss: 1.879801\n",
      "Train Epoch: 55 [25600/60000 (43%)]\tLoss: 1.906586\n",
      "Train Epoch: 55 [51200/60000 (85%)]\tLoss: 1.826749\n",
      "\n",
      "Test set: Average loss: -0.5538, Accuracy: 5552/10000 (56%)\n",
      "\n",
      "Train Epoch: 56 [0/60000 (0%)]\tLoss: 1.903291\n",
      "Train Epoch: 56 [25600/60000 (43%)]\tLoss: 1.879383\n",
      "Train Epoch: 56 [51200/60000 (85%)]\tLoss: 1.881057\n",
      "\n",
      "Test set: Average loss: -0.5547, Accuracy: 5558/10000 (56%)\n",
      "\n",
      "Train Epoch: 57 [0/60000 (0%)]\tLoss: 1.833441\n",
      "Train Epoch: 57 [25600/60000 (43%)]\tLoss: 1.903821\n",
      "Train Epoch: 57 [51200/60000 (85%)]\tLoss: 1.921514\n",
      "\n",
      "Test set: Average loss: -0.5553, Accuracy: 5567/10000 (56%)\n",
      "\n",
      "Train Epoch: 58 [0/60000 (0%)]\tLoss: 1.912250\n",
      "Train Epoch: 58 [25600/60000 (43%)]\tLoss: 1.920192\n",
      "Train Epoch: 58 [51200/60000 (85%)]\tLoss: 1.783683\n",
      "\n",
      "Test set: Average loss: -0.5554, Accuracy: 5567/10000 (56%)\n",
      "\n",
      "Train Epoch: 59 [0/60000 (0%)]\tLoss: 1.934896\n",
      "Train Epoch: 59 [25600/60000 (43%)]\tLoss: 1.979009\n",
      "Train Epoch: 59 [51200/60000 (85%)]\tLoss: 1.911937\n",
      "\n",
      "Test set: Average loss: -0.5555, Accuracy: 5565/10000 (56%)\n",
      "\n",
      "Train Epoch: 60 [0/60000 (0%)]\tLoss: 1.868055\n"
     ]
    }
   ],
   "source": [
    "lr = 0.02\n",
    "momentum = 0.9\n",
    "n_epochs = 100\n",
    "\n",
    "model = NetOneLayerLowRank(n_hidden=2**8, d=2, K=2)\n",
    "# model = NetOneLayer(n_hidden=2**8)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8301, 0.2546, 0.5547, 0.2650]],\n",
       "\n",
       "        [[0.4740, 0.0082, 0.6353, 0.9605]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2,1, 4)\n",
    "a\n",
    "F.max_pool1d(a, 4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Applies a 2D max pooling over an input signal composed of several input\n",
       "planes.\n",
       "\n",
       "In the simplest case, the output value of the layer with input size :math:`(N, C, H, W)`,\n",
       "output :math:`(N, C, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kH, kW)`\n",
       "can be precisely described as:\n",
       "\n",
       ".. math::\n",
       "\n",
       "    \\begin{equation*}\n",
       "    \\text{out}(N_i, C_j, h, w)  = \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1}\n",
       "                           \\text{input}(N_i, C_j, \\text{stride}[0] * h + m, \\text{stride}[1] * w + n)\n",
       "    \\end{equation*}\n",
       "\n",
       "If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides\n",
       "for :attr:`padding` number of points. :attr:`dilation` controls the spacing between the kernel points.\n",
       "It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.\n",
       "\n",
       "The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n",
       "\n",
       "    - a single ``int`` -- in which case the same value is used for the height and width dimension\n",
       "    - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n",
       "      and the second `int` for the width dimension\n",
       "\n",
       "Args:\n",
       "    kernel_size: the size of the window to take a max over\n",
       "    stride: the stride of the window. Default value is :attr:`kernel_size`\n",
       "    padding: implicit zero padding to be added on both sides\n",
       "    dilation: a parameter that controls the stride of elements in the window\n",
       "    return_indices: if ``True``, will return the max indices along with the outputs.\n",
       "                    Useful when Unpooling later\n",
       "    ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(N, C, H_{in}, W_{in})`\n",
       "    - Output: :math:`(N, C, H_{out}, W_{out})` where\n",
       "\n",
       "      .. math::\n",
       "          H_{out} = \\left\\lfloor\\frac{H_{in} + 2 * \\text{padding}[0] - \\text{dilation}[0]\n",
       "                * (\\text{kernel_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n",
       "\n",
       "          W_{out} = \\left\\lfloor\\frac{W_{in} + 2 * \\text{padding}[1] - \\text{dilation}[1]\n",
       "                * (\\text{kernel_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> # pool of square window of size=3, stride=2\n",
       "    >>> m = nn.MaxPool2d(3, stride=2)\n",
       "    >>> # pool of non-square window\n",
       "    >>> m = nn.MaxPool2d((3, 2), stride=(2, 1))\n",
       "    >>> input = torch.randn(20, 16, 50, 32)\n",
       "    >>> output = m(input)\n",
       "\n",
       ".. _link:\n",
       "    https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\anaconda3\\envs\\main-env\\lib\\site-packages\\torch\\nn\\modules\\pooling.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?nn.MaxPool2d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main-env)",
   "language": "python",
   "name": "main-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
